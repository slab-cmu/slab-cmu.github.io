<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <hr> <h1 id="hw-1-crowdsourced-annotations"><strong>HW 1: Crowdsourced Annotations</strong></h1> <ul> <li><strong>Due 11:59pm, Tuesday 2/15.</strong></li> <li>Zip the code and a PDF write-up into a single tar/zip file and submit through Canvas. Code will not be graded.</li> </ul> <h1 id="goals">Goals</h1> <p>Crowdsourcing annotations has become a fundamental aspect of NLP research. The goal of this assignment is to explore the ethical implications of soliciting crowdsourced data, specifically social biases that may emerge when asking for generated sentences.</p> <h1 id="overview">Overview</h1> <p>In this homework, you will perform a “bias audit” of an NLP dataset produced by crowdsourcing. You will attempt to measure the presence of social stereotypes in this dataset that may have harmful effects if used to train classifiers in downstream tasks.</p> <p>You will use pointwise mutual information (PMI) to find which associations are being made with identity labels. PMI can be used as a measure of word association in a corpus, i.e. how frequently two words co-occur above what might just be expected based on their frequencies. See the <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" rel="external nofollow noopener" target="_blank">PMI Wikipedia page</a> for more details. Here we use PMI to measure which words co-occur with labels for identities. This allows us to see associations that may perpetuate stereotypes.</p> <p>After this analysis, you will present specific examples from the data that you speculate could be particularly biased and problematic. In the optional advanced analysis, you will expand this analysis to another corpus.</p> <h1 id="data-and-resources">Data and Resources</h1> <ul> <li> <a href="https://nlp.stanford.edu/projects/snli/snli_1.0.zip" rel="external nofollow noopener" target="_blank">SNLI corpus</a>, a popular dataset for the NLP task of natural language inference. You can read more about the dataset and task on the <a href="https://nlp.stanford.edu/projects/snli/" rel="external nofollow noopener" target="_blank">SNLI website</a> or in the original paper (<a href="https://aclanthology.org/D15-1075/" rel="external nofollow noopener" target="_blank">Bowman et al. 2015</a>). <ul> <li>Use the training data CSV or JSON lines file (<code class="language-plaintext highlighter-rouge">snli_1.0_train.jsonl</code>) in the SNLI corpus. The <code class="language-plaintext highlighter-rouge">sentence1</code> column with premise that was supplied to annotators, and the <code class="language-plaintext highlighter-rouge">sentence2</code> column is the hypotheses that annotators came up with. See other details about the corpus format in the README supplied with SNLI.</li> </ul> </li> <li> <a href="/teaching/11-830/assignments/files/identity_labels.txt">List of identity labels</a> (based on <a href="https://aclanthology.org/W17-1609/" rel="external nofollow noopener" target="_blank">Rudinger et al. 2017</a>).</li> </ul> <h1 id="basic-requirements">Basic Requirements</h1> <p>Completing the basic requirements will earn a passing (B-range) grade.</p> <p><strong>Word association analysis:</strong> First, build a tool for calculating pointwise mutual information (PMI) between unigram frequencies in the SNLI dataset. Your tool should take a unigram, with word frequencies relative to a corpus, as input and give a list of other unigrams in the corpus ranked by PMI. Terms that occur less than 10 times in the corpus should not be considered; optionally you can consider other thresholds. For preprocessing, lowercase, remove stopwords and tokenize the data. Note that there are duplicate premises and hypotheses in the data; remove these and just look at unique utterances.</p> <p>Here’s how you can calculate PMI. Let \(c(w_i)\) be the count of word \(w_i\) in the corpus and \(c(w_i, w_j)\) be the number of times that \(w_i\) and \(w_j\) occur in the same premise or hypothesis. If they co-occur more than once within a premise or hypothesis, you can still just calculate that as one. With \(N\) as the number of documents (premises or hypotheses) in the corpus, we define \(P(w_i)\) as the word frequency \(c(w_i) / N\). Then PMI is:</p> \[PMI(w_i, w_j) = log_2 \frac{p(w_i, w_j)}{P(w_i)P(w_j)} = log_2\frac{N\cdot c(w_i, w_j)}{c(w_i)c(w_j)}\] <p>Compute PMI between the identity labels in the provided list and all other words in the SNLI training corpus (see details in the Data and Resources section above. Look at the top associated words for identity labels of your choice. Do you see any that may reflect social stereotypes? It is helpful to compare the top PMI words for certain identity terms with other related ones (such as <em>men</em> compared with <em>women</em>). Note that some terms in the list do not occur in the data; they are included for advanced analysis on possible other corpora.</p> <p>Calculate PMI separately for identity terms in the premises, which are the original provided captions from the Flickr30k image captioning dataset, and identity terms in the hypotheses, which were elicited in a crowdworking task. You will compare the associations made in the write-up.</p> <p><strong>Qualitative analysis:</strong> Find specific hypotheses from the dataset where an identity label occurs with a top-associated term that shows some social bias or does not. Look at 1-2 examples for at least 5 different identity labels. Also note the label (entailment, contradiction, neutral) and consider the impact of asking annotators for certain types of inference.</p> <p><strong>Crowdsourcing set-up:</strong> Read about what crowdworkers were asked to do in constructing the SNLI corpus in the SNLI paper. Come up with at least one idea about how the designers of the crowdsourcing task might have mitigated any social bias you found in your analysis. For example, are there certain topics that often led to biased hypotheses? Could the task have been structured differently or different instructions given to mitigate bias?</p> <h1 id="advanced-analysis">Advanced Analysis</h1> <p>Choose one of the options below for advanced analysis.</p> <h2 id="new-corpus">New corpus</h2> <p>Choose another crowdsourced NLP or ML dataset and perform a similar bias audit based on identity terms. Datasets to consider include (but are not limited to!):</p> <ul> <li> <a href="https://cims.nyu.edu/~sbowman/multinli/" rel="external nofollow noopener" target="_blank">MNLI</a>: The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. <a href="https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip" rel="external nofollow noopener" target="_blank">[download]</a> </li> <li> <a href="https://www.cs.rochester.edu/nlp/rocstories/" rel="external nofollow noopener" target="_blank">ROCStories</a>: ‘Story Cloze Test’ is a new commonsense reasoning framework for evaluating story understanding, story generation, and script learning. This test requires a system to choose the correct ending to a four-sentence story. To enable the Story Cloze Test, we created a new corpus of five-sentence commonsense stories, ‘ROCStories’. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. <a href="/teaching/11-830/assignments/files/rocstories_spring2016.csv">[download of original 2016 corpus]</a> </li> <li> <a href="https://aclanthology.org/P18-2124/" rel="external nofollow noopener" target="_blank">SQuAD 2.0</a> combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. <a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="external nofollow noopener" target="_blank">[data]</a> </li> <li> <a href="https://aclanthology.org/N19-1246/" rel="external nofollow noopener" target="_blank">DROP (QA)</a> is a QA dataset which tests comprehensive understanding of paragraphs. In this crowdsourced, adversarially-created, 96k question-answering benchmark, a system must resolve multiple references in a question, map them onto a paragraph, and perform discrete operations over them (such as addition, counting, or sorting). <a href="https://allennlp.org/drop" rel="external nofollow noopener" target="_blank">[data]</a> </li> </ul> <p>Modify the identity list by adding and/or dropping labels that do or do not occur above a frequency threshold in this new dataset and run the PMI word association analysis. Similar to the basic requirements, discuss stereotypes found in this corpus and give specific examples. Are there differences in the type or degree of stereotypes found compared with the SNLI corpus? Read about the annotation procedure for these datasets. How might these crowdsource tasks have set themselves up — or not — for responses that reflect stereotypes in ways that are similar or different from SNLI? Discuss potential implications for new crowdsourced data collection in the write-up.</p> <h2 id="phrases">Phrases</h2> <p>Expand the PMI analysis to higher-order n-grams and possibly syntactic phrases. Note that you will want to also expand the identity list to possibly include combinations of identity types (such as <em>asian man</em>). Refer to <a href="https://aclanthology.org/W17-1609/" rel="external nofollow noopener" target="_blank">Rudinger et al. (2017)</a> for ideas here.</p> <h2 id="bias-with-respect-to-class-labels">Bias with respect to class labels</h2> <p>In this assignment you implemented an approach for identifying associations between unigrams and identity labels in a corpus. You can similarly investigate whether identity words (and closely related words via e.g. embedding similarity) have some correlation with <em>class labels</em> in text classification tasks, such as <a href="https://nlp.stanford.edu/sentiment/index.html" rel="external nofollow noopener" target="_blank">sentiment analysis</a>. Discuss your results. What are possible implications of such bias for downstream uses of such a classifier? Are there certain model designs that might be more susceptible to such correlations than others?</p> <h2 id="association-measures">Association measures</h2> <p>Perhaps PMI is not the best lexical association measure to see social bias with identity terms. Explore lexical association measures other than PMI; see <a href="https://link.springer.com/article/10.1007/s10579-009-9101-4" rel="external nofollow noopener" target="_blank">Pecina (2010)</a> for ideas.</p> <h2 id="latest-methods">Latest methods</h2> <p>Find and implement a recent approach for identifying biases in datasets. Summarize the approach in a couple paragraphs, and how you applied it to SNLI or another dataset. Compare the results to your analysis based on PMI. What are trade-offs of each approach, in terms of implementation, efficiency, results?</p> <h1 id="write-up">Write-up</h1> <p>Each student should submit their own 2-3 page report (<a href="https://www.overleaf.com/latex/templates/acl-rolling-review-template/jxbhdzhmcpdm" rel="external nofollow noopener" target="_blank">ACL format</a>). Please do not submit more than 4 pages, though you can put large tables and figures in an appendix beyond that if necessary. The report should include:</p> <ul> <li>The top 5 terms associated with 10 identity labels of your choice by PMI for both premises and hypotheses in the SNLI dataset (variations on the same lemma such as man and men count separately). These terms can indicate social biases you see or not.</li> <li>A discussion of associations made in the SNLI dataset that are stereotypical, or a lack thereof. If you see any, discuss differences between premises and hypotheses regarding these stereotypes. Give specific examples of data where these associations are being made, and what potential harm that reinforcing this stereotype in a natural language inference dataset may have.</li> <li>A brief discussion of what steps may be taken to mitigate this effect when using crowdsourcing to create and annotate NLP datasets. For example, what instructions could be given to crowdworkers, and more importantly, how might a crowdsource task be structured to elicit fewer stereotypes? Do you think asking for free-form generated sentences from crowdworkers will always produce responses that reproduce stereotypes, or are there ways to give context that may lessen that effect? How much of any bias that you see is due to the original premises provided to annotators?</li> <li>[If doing advanced analysis] Results and discussion of advanced analysis.</li> </ul> <h1 id="grading-100-points">Grading (100 points)</h1> <ul> <li>20 points: Submitting assignment</li> <li>40 points: Completing basic requirements</li> <li>20 points: Write up is well-written, presents meaningful analysis, and contains all requested information</li> <li>20 points: Advanced analysis</li> </ul> <h1 id="references">References</h1> <ul> <li>Rachel Rudinger, Chandler May, &amp; Benjamin Van Durme. (2017). <a href="https://aclanthology.org/W17-1609/" rel="external nofollow noopener" target="_blank">Social Bias in Elicited Natural Language Inferences</a>. <em>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</em>, 74–79.</li> <li>Mor Geva, Yoav Goldberg, Jonathan Berant. (2019). <a href="https://anthology.aclweb.org/D19-1107/" rel="external nofollow noopener" target="_blank">Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets</a>. <em>Proceedings of EMNLP-IJCNLP</em>.</li> <li>Samantha Jaroszewski, Danielle Lottridge, Oliver L. Haimson, Katie Quehl. (2018). <a href="https://dl.acm.org/doi/10.1145/3173574.3173881" rel="external nofollow noopener" target="_blank">Genderfluid or Attack Helicopter: Responsible HCI Research Practice with Non-binary Gender Variation in Online Communities</a>. <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>. [link]</li> <li>Pavel Pecina. (2010). <a href="https://link.springer.com/article/10.1007/s10579-009-9101-4" rel="external nofollow noopener" target="_blank">Lexical association measures and collocation extraction</a>. <em>Language Resources &amp; Evaluation</em> 44:137–158.</li> <li>Florian Alexander Schmidt. (2013). <a href="https://ieeexplore.ieee.org/abstract/document/6686081" rel="external nofollow noopener" target="_blank">The good, the bad and the ugly: Why crowdsourcing needs ethics</a>. <em>Conference on Cloud and Green Computing</em> (CGC).</li> <li>Fabian L. Wauthier, Michael I. Jordan. (2011). <a href="https://papers.nips.cc/paper/2011/hash/0768281a05da9f27df178b5c39a51263-Abstract.html" rel="external nofollow noopener" target="_blank">Bayesian bias mitigation for crowdsourcing</a>. <em>Advances in Neural Information Processing Systems</em>.</li> </ul> </body></html>