<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
    <!-- Metadata, OpenGraph and Schema.org -->
    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Emma  Strubell | 11-830: Computational Ethics for NLP</title>
    <meta name="author" content="Emma  Strubell" />
    <meta name="description" content="Description" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />
    <!-- Bootstrap & MDB -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />
    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />
    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⛰️</text></svg>">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="https://strubell.github.io/teaching/11-830/assignments/hw3/">
</head>
<!-- Body -->
<body class="fixed-top-nav">
  <!-- Content -->
  <div class="container mt-5">
    <header class="post-header">
      <h1 class="post-title"><strong><a href="/teaching/11-830/">11-830: Computational Ethics for NLP</a></strong></h1>
      <h2 class="post-semester">
<strong>Spring 2022</strong>
    </h2>
    <ul>
      <li>
<strong>Time:</strong> 10:10-11:30 Tuesdays &amp; Thursdays</li>
      <li>
<strong>Place:</strong> Singleton Room, Roberts Engineering Hall <a href="/assets/pdf/Campus-Map-to-Singleton.pdf">[PDF map]</a>
</li>
      <li>
<strong>Zoom:</strong> See <a href="https://canvas.cmu.edu/courses/28069" target="_blank" rel="noopener noreferrer">Canvas</a> or <a href="http://11830workspace.slack.com" target="_blank" rel="noopener noreferrer">Slack</a> for Zoom link. </li>
    </ul>
  </header>
  <hr>
  <h1 id="hw-3-privacy-and-obfuscation"><strong>HW 3: Privacy and Obfuscation</strong></h1>
  <ul>
    <li><strong>Due 11:59pm, Thursday 3/31.</strong></li>
    <li>
<strong>Submission:</strong> Submit a zip/tar file to Canvas. Include your write-up (titled <code class="language-plaintext highlighter-rouge">FirstName_LastName_hw3.pdf</code>) and a folder containing your code. Code will not be graded.</li>
  </ul>
  <hr>
  <h2 id="goals">Goals</h2>
  <p>Online data has become an essential source of training data for natural language processing and machine learning tools; however, the use of this type of data has raised concerns about privacy. Furthermore, the detection of demographic characteristics is a common component of microtargeting. In this assignment, you will explore how to obfuscate demographic traits, specifically gender. The primary goals are (1) develop a method for obfuscating an author’s gender and (2) explore the trade-off between obfuscating an author’s identity and preserving useful information in the data</p>
  <hr>
  <h2 id="overview">Overview</h2>
  <p>The data for this assignment is available <a href="https://drive.google.com/file/d/124NOt4EcDMtMB27sOn17hpzRmpssukuR/view?usp=sharing" target="_blank" rel="noopener noreferrer">here</a>. Your primary dataset consists of posts from Reddit. Each post is annotated with the gender of the post’s author (<code class="language-plaintext highlighter-rouge">op_gender</code>) and the subreddit where the post was made (<code class="language-plaintext highlighter-rouge">subreddit</code>). The main text of the post is in the column <code class="language-plaintext highlighter-rouge">post_text</code>. The contents of the provided data include:</p>
  <ul>
    <li>
<code class="language-plaintext highlighter-rouge">classify.py</code>: a classifier that predicts the author’s gender and the subreddit for a post (example run: <code class="language-plaintext highlighter-rouge">python classify.py --test_file dataset.csv</code>). Note that this file also uses the two provided pickle files.</li>
    <li>
<code class="language-plaintext highlighter-rouge">dataset.csv</code>: your primary data.</li>
    <li>
<code class="language-plaintext highlighter-rouge">background.csv</code>: additional Reddit posts that you may optionally use for training an obfuscation model. A larger version is available <a href="https://drive.google.com/file/d/1n5-0ePZOxNASUzcP9CUG_tIjpSog09EQ/view?usp=sharing" target="_blank" rel="noopener noreferrer">here</a>.</li>
    <li>
<code class="language-plaintext highlighter-rouge">female.txt</code>: a list of words commonly used by women.</li>
    <li>
<code class="language-plaintext highlighter-rouge">male.txt</code>: a list of words commonly used by men.</li>
  </ul>
  <p>The provided classifier achieves an accuracy of 64.95% at identifying the gender of the poster and an accuracy of 85.85% at identifying a post’s subreddit when tested over dataset.csv. Your goal in this assignment is to obfuscate the data in dataset.csv so that the provided classifier is unable to determine the gender of authors, while still being able to determine the subreddit of the post. Note that in this set-up, we treat the provided classifier as a blackbox adversary (please do not try to hack it). This assignment was largely inspired by the paper <em>Obfuscating Gender in Social Media Writing</em> (<a href="https://aclanthology.org/W16-5603/" target="_blank" rel="noopener noreferrer">Knight &amp; Reddy, 2016</a>), which may be a useful reference. Scenerios where this obfuscation model might be useful could be social media users who want to preserve their privacy by hiding their gender from the adversary, without losing the meaning of their post. You could also imagine this is a dataset of health records or other sensitive information that needs to be anonymized before providing it to NLP researchers.</p>
  <hr>
  <h2 id="basic-requirements">Basic Requirements</h2>
  <p>Completing the basic requirements will earn a passing (B-range) grade</p>
  <p><strong>First</strong>, build a baseline obfuscation model:</p>
  <ul>
    <li>For each post in <code class="language-plaintext highlighter-rouge">dataset.csv</code>, if the post was written by a man (<code class="language-plaintext highlighter-rouge">M</code>) and it contains words from <code class="language-plaintext highlighter-rouge">male.txt</code>, replace these words with a random word from <code class="language-plaintext highlighter-rouge">female.txt</code>.</li>
    <li>Obfuscate posts written by women (<code class="language-plaintext highlighter-rouge">W</code>) in the same way (i.e. by replacing words from <code class="language-plaintext highlighter-rouge">female.txt</code> with random words from <code class="language-plaintext highlighter-rouge">male.txt</code>)</li>
    <li>Test <code class="language-plaintext highlighter-rouge">classify.py</code> on your obfuscated data and analyze the results.</li>
  </ul>
  <p><strong>Second</strong>, improve your obfuscation model:</p>
  <ul>
    <li>Instead of replacing words from <code class="language-plaintext highlighter-rouge">male.txt</code> with randomly chosen words from <code class="language-plaintext highlighter-rouge">female.txt</code>, choose a semantically similar word from <code class="language-plaintext highlighter-rouge">female.txt</code> (use the same metric for replacing words from <code class="language-plaintext highlighter-rouge">female.txt</code> with words from <code class="language-plaintext highlighter-rouge">male.txt</code>). You may use any metric you choose for identifying semantically similar words. We recommend using cosine distance between pre-trained word embeddings (available <a href="http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/" target="_blank" rel="noopener noreferrer">here</a>). You can also use SpaCy-based similarity <a href="https://spacy.io/usage/linguistic-features" target="_blank" rel="noopener noreferrer">here</a> (<a href="https://ashutoshtripathi.com/2020/09/04/word2vec-and-semantic-similarity-using-spacy-nlp-spacy-series-part-7/" target="_blank" rel="noopener noreferrer">example 1</a>, <a href="https://www.geeksforgeeks.org/python-word-similarity-using-spacy/" target="_blank" rel="noopener noreferrer">example 2</a>).</li>
    <li>Test <code class="language-plaintext highlighter-rouge">classify.py</code> on data obfuscated using your improved model and analyze the results. The classifier should perform close to random at identifying gender (e.g. &lt;53.5%) and should obtain at least 79% accuracy on classifying the subreddit.</li>
  </ul>
  <p><strong>Third</strong>, experiment with some basic modifications to your obfuscation models. For example, what if you randomly decide whether or not to replace words instead of replacing every lexicon word? What if you only replace words that have semantically similar enough counterparts?</p>
  <h2 id="advanced-analysis">Advanced Analysis</h2>
  <p>Develop your own obfuscation model. We provide <code class="language-plaintext highlighter-rouge">background.csv</code>, a large data set of Reddit posts tagged with gender and subreddit information that you may use to train your obfuscation model. A larger version of the background corpus is available <a href="https://drive.google.com/file/d/1n5-0ePZOxNASUzcP9CUG_tIjpSog09EQ/view?usp=sharing" target="_blank" rel="noopener noreferrer">here</a>. Your ultimate goal should be to obfuscate text so that the classifier is unable to determine the gender of an author (no better than random guessing) without compromising the accuracy of the subreddit classification task. However, creative or thorough approaches will receive full credit, even if they do not significantly improve results. Some ideas you may consider:</p>
  <ul>
    <li>Develop your own lexicons using <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" target="_blank" rel="noopener noreferrer">pointwise mutual information scores</a> or <a href="https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863" target="_blank" rel="noopener noreferrer">log odds with a dirichlet prior</a>.</li>
    <li>Follow the procedure described in <a href="https://aclanthology.org/W16-5603/" target="_blank" rel="noopener noreferrer">“Obfuscating Gender in Social Media Writing”</a>.</li>
    <li>Use an adversarial objective as described in <a href="https://www-nlp.stanford.edu/pubs/pryzant2017sigir.pdf" target="_blank" rel="noopener noreferrer">“Predicting Sales from the Language of Product Descriptions”</a> to train a model that is good at predicting subreddit classification but bad a predicting gender. The key idea in this approach is to design a model that does not encode information about protected attributes (in this case, gender).</li>
    <li>Use a model for style transfer to generate the text, such <a href="https://arxiv.org/abs/1804.09000" target="_blank" rel="noopener noreferrer">“Style Transfer Through Back-Translation”</a>.</li>
  </ul>
  <p>In your report, include a description of your model and results.</p>
  <h2 id="extra-credit">Extra Credit!</h2>
  <ul>
    <li>Perform multiple advanced analyses</li>
    <li>Perform a new advanced analysis not suggested above, or perform one of the last 3 advanced analyses. Instead of simply describing your approach, provide a detailed and clear motivation, description, and analysis, including a comparison to the basic analysis.</li>
  </ul>
  <hr>
  <h2 id="write-up">Write-up</h2>
  <p>Write a 2-3 page report (ACL format) <code class="language-plaintext highlighter-rouge">FirstName_LastName_hw3.pdf</code>. Please do not write more than 4 pages. The report should include:</p>
  <ul>
    <li>Description of baseline, improved and advanced (if completed) obfuscated models.</li>
    <li>Description of the experiments you tried with your improved obfuscation model.</li>
    <li>Results for your models by using them to obfuscate <code class="language-plaintext highlighter-rouge">dataset.csv</code> and running <code class="language-plaintext highlighter-rouge">classify.py</code> over your obfuscated test data.</li>
    <li>Qualitative examples of text obfuscated with your models.</li>
    <li>A brief discussion of the ethical implications of obfuscation and privacy that draws from concepts covered during lecture.</li>
  </ul>
  <hr>
  <h2 id="grading-100-points--up-to-10-extra-credit">Grading (100 points + up to 10 extra credit)</h2>
  <ul>
    <li>20 points - Submitting assignment.</li>
    <li>40 points - Completing basic requirements.</li>
    <li>20 points - Write up is well-written, presents meaningful analysis, and contains all requested information.</li>
    <li>20 points - Advanced analysis.</li>
    <li>10 points - Extra credit.</li>
  </ul>
</div>
</body>
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>
<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
<script src="/assets/js/common.js"></script>
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
</html>
